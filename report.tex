\documentclass[english,a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage[english]{babel}
\usepackage{lmodern}
\usepackage{hyperref}
\usepackage{color}
\usepackage{url}
\usepackage{verbatim} 

\begin{document}

\title{Click-stream Data Analysis of Web Traffic}
\author{Riivo Kikas, Karl Potisepp}
\date{\today}
\maketitle

\begin{abstract}
In  this paper, we will conduct a case study of mining frequent user access patterns from web log files.
\end{abstract}





\section{Introduction} 
Understanding how users navigate and browse through web sites is important in many aspects, such as content recommendation,
personalization and targeted advertising. Even more, it helps to understand users' needs and helps and provides real world input to better organization of content and structure.

The aim of this report is to illustrate pattern mining from web logs with example of web access logs from the public web site of the Faculty of Mathematics and Computer Science of the University of Tartu\footnote{\url{http://math.ut.ee}}. Motivation for this study was to find meaningful use-cases from log files and identify possible bottlenecks in the presentation layer and provide some input for user interface redesign. 


The primary objective was to discover the most frequent sequences by analyzing the visitors' browsing sessions. The nature of pattern mining carried out was mainly exploratory, concentrating on frequent item sets and sequence mining.

Write here about sessions and data. And the overview of the process








\section{Data preparation} 
For each time an user visits a web page, a piece of information is typically stored in a web server's log file. When user navigates around a website, all such pieces of information (alternatively called requests or clicks) form a session (click-stream). We consider a session as an user's single visit to the website and associate each click with to one session.

Input data is stored in website log files, namely in Common Log Format\cite{ref_clf} format. Each log entry consists of :(i) User's IP address, (ii) Access time, (iii) Request method (\emph{GET}, \emph{POST}), etc), (iv) URL of the page accessed, (v) Prototcol (typically HTTP/1.0), (vi) Return code, (vii) Number of bytes transmitted\ref{log_sample}.

It must be mentioned, that log files contain all HTTP requests made to the web server, including requests for downloading images and style sheet documents attached to the web pages served.

\begin{figure}[h]
{\tiny
\begin{verbatim}
192.168.1.245 - - [13/Sep/2009:04:07:56 +0300] "GET /ati/struktuur/tiiger HTTP/1.0" 200 19828
192.168.1.215 - - [13/Sep/2009:04:09:30 +0300] "GET / HTTP/1.0" 200 13511
192.168.1.215 - - [13/Sep/2009:04:12:02 +0300] "GET /103369 HTTP/1.0" 200 17367
192.168.1.215 - - [13/Sep/2009:04:14:40 +0300] "GET /varia/itinfo HTTP/1.0" 200 19411
\end{verbatim}
}
\label{log_sample}
\caption{Example of log file}
\end{figure}

Two important tasks must be performed before clickstream data from \emph{CLF} files can be used for analyzing: data cleaning and session identification.









\subsection{Data cleaning}
Since log files contain all requests made to the server, we need to extract only relevant requests and eliminate others. When filtering requests from log files, following rules were applied:

\begin{itemize}
\item Only requests to the public site are considered. Requests to personal homepages (pages starting with ~) are discarded. Also, all static content files that were used by web pages were removed, such as image files, JavaScript and CSS documents. \\ The server also had had special error page url, that the user was redirected to when they tried to access a non-existant resource. We eliminated this kind of access requests, as these are not initially made by users. It must be noted that the request for the original page that caused the server to redirect the user remains in the log file.
  
\item All other HTTP request types besides POST and GET were ignored. We noticed that there were regular OPTIONS request, that most probably were made by some automated monitoring tool. 
  
\item We composed a blacklist of of IP addresses, whose requests were all ignored. An IP address was added to blacklist, if there was a request made from the IP address to access \emph{robots.txt} file. Namely, we believe that only automated indexers request robot.txt and therefore we can ignore all subsequent requests from those IP-s, since we are only interested in how a human user perceives the web site. 
\end{itemize}


In addition to that, we modified requested URLs by removing query string attributes(the part of the URL that starts with ?). We assumed that typically URLs identify a distinct page and query-string some operation or action on it. This helped us to reduce number of different URLs while preserving meaningful information about the usage patterns.

After performing all the procedures described above, the log files now contain only information about the HTTP requests originating from intentional clicks made by the users on pages. These requests can be grouped into users' click-streams.











\subsection{Session identification}
The next step was to group user requests made during one visit into sessions. A user session is defined as a sequence of temporally compact accesses by a user \cite{on_mining_logs}.

Generally, this can be done by providing a cookie to each user with some generated unique identification number and then logging each request and the cookie ID simultaneously. However since log files in CLF do not store an explicit session ID, and since we could not modify the existing software stack, we had to use the data that was already given.  

As a solution, we used a timeout based method to group requests into sessions. We consider a sequence of requests from the same IP address ordered by time to be in
same session, if the time between consecutive requests (clicks) is not more
than $t = 30$ minutes. If time between requests exceeds $t$, we
consider these requests to be part of a new session. While this method is
probably not the best, we consider it sufficient as it mimics the way session
timeouts are measured on many web sites. We also consider it reasonable to
assume that if an user has not interacted with the server for more than $t$
minutes, any new interaction from an user from the same IP address can be
thought of as a new use case scenario.










\subsection{Statistics about the logs} 
The log files contained info about requests made to the server from August to December 2009, with October data missing. In total, log files contained 
$??$ requests, of which $??$ remained after data cleaning. These $??$ requests were done in $??$ sessions. Average session length was $??$. Requests involved $??$ distinct URLs. From figure ??, it can be seen that ...

Graph:Session length distribution
Graph: page counts?
















\section{Frequent item sets} 
From the perspective of web site design and browsing sessions, the most important question is whether or not there are groups of pages users tend to visit together. If these do exist, a web designer can smooth users' browsing experience by providing easier access to the information users actually seek. This can be achieved by eliminating unnecessary 'gateway' pages (i.e. instead of pointing at a sign that points to the lavatory, point straight to where the lavatory is) and merging contents of the pages that are often viewed simultaneously. Common sense must of course be applied in this process in order to avoid tyranny of the majority - prominently placing the phone numbers of the Computer Science department and hiding all the other departments' can lead to negative results instead. However this report deals not with web design, but rather finding areas on a web site where it can be applied.

We approached the task of identifying these groups of pages by thinking of every page as a unique \emph{item}, which then form sequences depicting the progress of users navigating through the web site. Our objective was to find if there are some sequences (or parts of sequences) that tend to show up often in users' movements. However looking at the number of browsing sessions found in the data, it quickly becomes clear that a way to decide whether or not a sequence is significant is needed. It is obvious that if a common sequence is found in only three or four out of a total amount of 2,000 sessions, no important knowledge is gained. Therefore a threshhold is needed to 
eliminate the sequences that are not frequently found. To do this we use a tool often used in frequent item set mining \cite{frequent_item_set_mining} called \emph{support}.

Support is a number that shows how frequently a sequence appears in the data. There are two kinds of support: absolute and relative. Absolute support is the exact number of times the sequence is present, relative support is the percentage of all sessions that contain the sequence. Using this tool, it is trivial to filter out sequences that are not common in the data. The only problem is that there is no magic bullet solution to choosing the 'right' support threshhold.

First and foremost interesting question about web session on can ask, are there some pages that users tend to visit together during one session?

Describe algorithm used. Used closed frequent itemsets
What are problems with frequent item sets: no  repeating elements. order
Support selection
Graph: support vs itemset count





\section{Frequent sequential patterns}

Frequent item give basic insight about which pages are visited together during one session. But it has limitations. It does not take into count order of pages in which pages were visited, which is very important information in web usage mining. Second biggest limitation, it does not give information wheter on page was visited multiple times. This helps to identify how users actually traverse on the website.

To extract frequent sequential patterns

Graph: support vs itemset count
What can we see from this.





\subsection{Time factor}
Sequntial patterns can be used to see how much user spend time on each pages. Suppose we have some kind of frequent pattern, where user traverses following path: $page1 \rightarrow page2 \rightarrow page3$. For this pattern, we can search all transaction where it occurs and calculate times













\section{Maximal forward references}

During user navigation, user tend to to back links e.g. go back to page that they have visited before. In some cases these back links can be considered only as navigational feature, not because of user was interested the content.  When we eliminate the back references from web session, we can construct such clickstream that show users target page e.g. user interest.

backward refernces
apriori

We discovered:

Compared to sequential rules:








\section{Evolving patterns through time} 
One interesting aspects of log mining is using temporal data. We carried out experiments, to see how patterns change thorough time. When mining school website data, we can consider some periods of interets. August is the pre semester period and students might be looking for information about new professors, timetables, good semester provided upcoming semester. Now when semester starts, it can be assumed that these kinds of aims become outdate and new bevahioural patterns. And again, in decemeber, it is the period of exams and in our case also the period when students must choose their final year project topic and supervisor. To test these hypothesis, we concluded following test. 

We split up the data set into three.

* August
* Sepetember
* December

For each period, we extraced sequential frequnt itemsets:




It can be concluded, that.








\section{Patterns restriction by user intention}

sven idea. reduce sparsity 

When minign data with preprovided supprt information

From previous study we saw that there are people who visit pages AAA, BB, CC. These can be considered the people looking for specific information. We concluded an experiment:
Eliminated all sessions that do no contain pages AAA, BB, CCC.
Mine frequent patterns from remaining session.
















\section{Summary}
Methods provided will not scale in web2.0 environments.
We have shown practical example how web analytics can be taken beyond google analytics and webalizer










\section*{References}
\bibliographystyle{alpha}
\bibliography{bibliography}
\end{document}
