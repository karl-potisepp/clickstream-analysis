\documentclass[12pt, english,a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage[english]{babel}
\usepackage{lmodern}
\usepackage{hyperref}
\usepackage{color}
\usepackage{url}
\usepackage{verbatim} 

\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}


\graphicspath{{./figures/}}
\begin{document}


\title{Click-stream Data Analysis of Web Traffic}
\author{Riivo Kikas, Karl Potisepp}
\date{\today}
\maketitle


\begin{abstract}
In this paper, we conduct a case study of mining frequent user access patterns from web log files. Our primary objective was to discover the most frequent browsing patterns by analyzing the visitors' browsing sessions. The nature of pattern mining carried out was mainly exploratory, concentrating on frequent item sets and sequence mining. To achieve this, we eliminated garbage data from the access logs, grouped the remaining data into sessions and searched them for interesting usage patterns. During this work we experimented with several frequent item set mining algorithms and tools. As a result of our work we provide this case study and some statistics about the usage patterns of the public web site of the Faculty of Mathematics and Computer Science of the University of Tartu.
\end{abstract}


\tableofcontents


\section{Introduction} 
Understanding how users navigate and browse through web sites is important in many aspects, such as content recommendation,
personalization and targeted advertising. Even more, it helps to understand users' needs and helps and provides real world input to better organization of content and structure. Having on hand the web server log files of a web site can provide important insight to the users' behavior, because from this data one can reconstruct a reasonably accurate overview of how users navigate their way around. Coupled with data mining algorithms it is therefore probable that important knowledge can be gained.

As an experimental case study, we studied the web access logs from the public web site of the Faculty of Mathematics and Computer Science of the University of Tartu\footnote{\url{http://math.ut.ee}}. Our aim was to try and find meaningful use-cases from log files and identify possible bottlenecks in the presentation layer and provide some input for user interface redesign.





\section{Data preparation} 
For each time an user visits a web page, a piece of information is typically stored in a web server's log file. When user navigates around a website, all such pieces of information (alternatively called requests or clicks) form a session (click-stream). We consider a session as an user's single visit to the website and associate each click with to one session.

Input data is stored in website log files, namely in Common Log Format\cite{ref_clf} format. Each log entry consists of :(i) User's IP address, (ii) Access time, (iii) Request method (\emph{GET}, \emph{POST}), etc), (iv) URL of the page accessed, (v) Prototcol (typically HTTP/1.0), (vi) Return code, (vii) Number of bytes transmitted\ref{log_sample}.

It must be mentioned, that log files contain all HTTP requests made to the web server, including requests for downloading images and style sheet documents attached to the web pages served.

\begin{figure}[h]
{\tiny
\begin{verbatim}
192.168.1.245 - - [13/Sep/2009:04:07:56 +0300] "GET /ati/struktuur/tiiger HTTP/1.0" 200 19828
192.168.1.215 - - [13/Sep/2009:04:09:30 +0300] "GET / HTTP/1.0" 200 13511
192.168.1.215 - - [13/Sep/2009:04:12:02 +0300] "GET /103369 HTTP/1.0" 200 17367
192.168.1.215 - - [13/Sep/2009:04:14:40 +0300] "GET /varia/itinfo HTTP/1.0" 200 19411
\end{verbatim}
}
\label{log_sample}
\caption{Example of a log file}
\end{figure}

Two important tasks must be performed before clickstream data from \emph{CLF} files can be used for analyzing: data cleaning and session identification.







\subsection{Data cleaning}
Since log files contain all requests made to the server, we need to extract only relevant requests and eliminate others. When filtering requests from log files, following rules were applied:

\begin{itemize}
\item Only requests to the public site are considered. Requests to personal homepages (pages starting with ~) are discarded. Also, all static content files that were used by web pages were removed, such as image files, JavaScript and CSS documents. \\ The server also had had special error page url, that the user was redirected to when they tried to access a non-existant resource. We eliminated this kind of access requests, as these are not initially made by users. It must be noted that the request for the original page that caused the server to redirect the user remains in the log file.
  
\item All other HTTP request types besides POST and GET were ignored. We noticed that there were regular OPTIONS request, that most probably were made by some automated monitoring tool. 
  
\item We composed a blacklist of of IP addresses, whose requests were all ignored. An IP address was added to blacklist, if there was a request made from the IP address to access \emph{robots.txt} file. Namely, we believe that only automated indexers request robot.txt and therefore we can ignore all subsequent requests from those IP-s, since we are only interested in how a human user perceives the web site. 
\end{itemize}


In addition to that, we modified requested URLs by removing query string attributes(the part of the URL that starts with ?). We assumed that typically URLs identify a distinct page and query-string some operation or action on it. This helped us to reduce number of different URLs while preserving meaningful information about the usage patterns.

After performing all the procedures described above, the log files now contain only information about the HTTP requests originating from intentional clicks made by the users on pages. These requests can be grouped into users' click-streams.











\subsection{Session identification}
The next step was to group user requests made during one visit into sessions. A user session is defined as a sequence of temporally compact accesses by a user \cite{on_mining_logs}.

Generally, this can be done by providing a cookie to each user with some generated unique identification number and then logging each request and the cookie ID simultaneously. However since CLF does not store an explicit session ID, and since we could not modify the existing software stack, we had to use the data that was already given.  

As a solution, we used a timeout based method to group requests into sessions. We consider a sequence of requests from the same IP address ordered by time to be in same session, if the time between consecutive requests (clicks) is not more
than $t = 30$ minutes. If time between requests exceeds $t$, we consider these requests to be part of a new session. While this method is probably not the best, we consider it sufficient as it mimics the way session timeouts are measured on many web sites. We also consider it reasonable to assume that if an user has not interacted with the server for more than $t$
minutes, any new interaction from an user from the same IP address can be thought of as a new use case scenario.







\subsection{Statistics about the logs} 
The log files contained info about requests made to the server from August to December 2009, with October data missing. In total, log files contained 
$544237$ requests, of which $122822$ remained after data cleaning. These requests were grouped to $35952$ sessions. Average session length was $3.4$ clicks. Distribution of user session length can be see on figure \ref{session_len}. Requests involved $2059$ distinct URLs. 

\begin{figure}[H]
  \centering
      \includegraphics[width=0.9\textwidth]{session_len}
  \caption{Session length histogram}
  \label{session_len}
\end{figure}









\section{Frequent item sets} 
From the perspective of web site design and browsing sessions, the most important question is whether or not there are groups of pages users tend to visit together. The presence of such kinds of groups can give a web designer valuable insight into how people use the web site. For example, content split on separate pages but still often viewed together can be merged to achieve a smoother browsing experience.

We approached the task of identifying such groups by thinking of every page as a unique \emph{item}, which then form sets, each containing pages visited during one browsing session. To find pages that are frequently visited together, we searched among these sets for subsets of pages that frequently appear together. The problem now was deciding when a subset of pages is frequent. We solved this by using a set mining \cite{frequent_item_set_mining} concept called \emph{support}.

Support is a number that shows how frequently a subset appears in all item sets. Using this concept, it is trivial to eliminate subsets that are not common. The only problem is that there is no magic bullet solution to choosing the 'right' support threshold. A value too high does not provide insight into more specific subsets, a value too low results in too much non-frequent garbage.
\begin{figure}[H]
  \centering
      \includegraphics[width=0.9\textwidth]{apriori_closed_itemset_count}
  \caption{How rule support changes number of frequent closed itemsets}
\end{figure}


However even if we manage to choose an optimal value for the support threshhold, after filtering out all the infrequent subsets, some redundancy remains in the results. It is easy to see that if a set of 5 pages appears frequently throughout the sessions, all 4 page subsets of that set appear at least as frequently. To eliminate this kind of noise, we used the Apriori algorithm \cite{apriori} to find \emph{closed itemsets}.

In addition to the difficulties in choosing an ideal support threshold, there are other problems with this approach to detecting pages frequently requested during one session. While we can easily find groups of pages that are probably viewed together, much more insight could be gained if we knew the order in which the pages were navigated. Knowing this we would also gain information on how the users find what they need on the site. Similarly, if an user visits a page several times during one session, this fact is not known to us. Using \emph{sequential item sets} solves these problems.







\section{Frequent sequential patterns}
Simply by listing all frequent item sets we will not see much of how users actually use the website. This is because of two reasons. First, frequent item sets do not capture the order of pages visited. Secondly, information about multiple page visits is lost constructing item sets.

The solution to this problem can be solved by mining frequent sequential item sets. Sequential pattern mining is about mining frequent event sequences from databases, which means that we are searching for sequences of clicks that more often than not are followed by each other. Frequent sequences can be used to see how users actually tend to navigate and reach some desired resource.

To mine frequent patterns from our dataset, we used an \emph{a priori}-based frequent item sequence miner that uses a trie to store the candidates\cite{seq_apriori}.

\begin{figure}[H]
  \centering
      \includegraphics[width=0.9\textwidth]{sequential_itemset_count}
  \caption{Number of frequent sequential patterns depending on relative support}
\end{figure}

\begin{figure}[H]
  \centering
\begin{tabular}{ r | l }
Support & frequent sequential pattern \\ \hline
0.63 & $ \overset{17.0}{avaleht} $ \\ \hline
0.38 & $ \overset{513.0}{avaleht} \rightarrow \overset{137.0}{avaleht} $ \\ \hline
0.16 & $ \overset{592.5}{avaleht} \rightarrow \overset{600.5}{avaleht} \rightarrow \overset{237.5}{avaleht} $ \\ \hline
0.16 & $ \overset{28.0}{ati} $ \\ \hline
0.13 & $ \overset{11.0}{15213} $ \\ \hline
0.1 & $ \overset{9.5}{15213} \rightarrow \overset{23.0}{15213} $ \\ \hline
0.1 & $ \overset{4.0}{inimesedtudengid} $ \\ \hline
0.09 & $ \overset{4.0}{inimesedInstituudid} $ \\ \hline
0.09 & $ \overset{42.0}{15208} $ \\ \hline
0.09 & $ \overset{110.5}{avaleht} \rightarrow \overset{117.0}{15213} $ \\ \hline
0.09 & $ \overset{8.0}{avaleht} \rightarrow \overset{4.0}{inimesedtudengid} $ \\ \hline
0.09 & $ \overset{255.0}{ati} \rightarrow \overset{179.5}{ati} $ \\ \hline
0.09 & $ \overset{21.0}{15205} $ \\ \hline
0.08 & $ \overset{4.0}{inimesedtudengid} \rightarrow \overset{8.0}{15213} $ \\ \hline
0.08 & $ \overset{7.0}{avaleht} \rightarrow \overset{4.0}{inimesedInstituudid} $ \\ \hline
0.08 & $ \overset{429.0}{avaleht} \rightarrow \overset{474.0}{avaleht} \rightarrow \overset{401.0}{avaleht} \rightarrow \overset{171.5}{avaleht} $ \\ \hline
0.08 & $ \overset{7.0}{avaleht} \rightarrow \overset{4.0}{inimesedtudengid} \rightarrow \overset{7.5}{15213} $ \\ \hline
0.08 & $ \overset{81.0}{avaleht} \rightarrow \overset{67.0}{15213} \rightarrow \overset{69.0}{15213} $ \\ \hline
0.07 & $ \overset{97.5}{avaleht} \rightarrow \overset{73.0}{15208} $ \\ \hline
0.07 & $ \overset{4.0}{inimesedtudengid} \rightarrow \overset{7.0}{15213} \rightarrow \overset{18.0}{15213} $ \\ \hline
0.07 & $ \overset{4.0}{inimesedInstituudid} \rightarrow \overset{31.5}{15208} $ \\ \hline
0.07 & $ \overset{8.0}{avaleht} \rightarrow \overset{4.0}{inimesedtudengid} \rightarrow \overset{7.0}{15213} \rightarrow \overset{18.0}{15213} $ \\ \hline
0.07 & $ \overset{8.0}{avaleht} \rightarrow \overset{19.0}{15205} $ \\ \hline
0.07 & $ \overset{6.0}{avaleht} \rightarrow \overset{3.0}{inimesedInstituudid} \rightarrow \overset{30.0}{15208} $ \\ \hline
0.07 & $ \overset{9.0}{15213} \rightarrow \overset{20.0}{15213} \rightarrow \overset{7.0}{15213} $ \\ \hline
0.06 & $ \overset{59.5}{oppetootunniplaanid} $ \\ \hline
\end{tabular}
  \caption{Frequent sequential patterns with time decorations. Time is given in seconds and is calculated by taking median value of time spent on that page that contains specified rule. }
  \label{frequent_patterns}
\end{figure}


\subsection{Page access times}
Because the log files contain time and date info for every request made by the user, we know the time of each mouse click the user made (to a precision of 1-2 seconds, taking into account the time it takes for a request to reach the server). Knowing this we can find how much time users actually spend on a web page by calculating the time interval between two clicks. For every page, we can calculate the average time users spend on this page. However a more effective approach is to apply this method to previously found sequential patterns. Namely, for every frequent sequence found, we find all sessions that contain this sequence and calculate median time spent viewing each page in that sequence. We chose median time as we consider this more appropriate for experimental evaluation.

This method allows us to determine the pages that are visited only because of navigation and not for content. For usability reasons, such pages should be removed to reduce user clicks.





\section{Maximal reference sequences}
Path traversal pattern mining has been studied extensively to find better alternatives to regular frequent sequence mining in the context of web mining. A solution has been proposed \cite{path_patterns} that provides regular sequential patterns, but reduces the patterns only to \emph{forward reference sequences}, that is patterns that only contain forward navigations. Authors of \cite{path_patterns} say that forward references illustrate what people are looking for (e.g. destination pages) and eliminate back references from sessions.

When we applied this algorithm to our dataset, we found almost the same frequent patterns as with the regular sequential pattern mining technique. The difference was only the number of patterns found, which means that by eliminating back references and repeating pages, we find less patterns. When the number of patterns is bigger, this method helps to achieve better overview of frequent user traversal paths and target pages than the previously used method.

\section{Changes in patterns through time} 
Pattern mining can also give insight into how patterns change over time. When mining school website data, one can hypothesize how users' interest changes in time. We can presume, that since August is the pre-semester period and students might be looking for information about new professors, timetables or good seminars provided on the upcoming semester. Once the semester starts in September, it can be assumed that these kinds of aims become outdated and new behavioral patterns arise.

We split up the data set into two: August and September and compared sequential rules found for both datasets with same relative support, s=$0.04\%$.


\section{Restricting patterns by user specified pages}

In our data set, the number of different pages visited is relatively big compared to the total number of sessions and average session length, and many interesting frequent patterns may not be visible without lowering the support threshhold too much. One idea how to overcome this problem and easily find more appropriate and significant frequent patterns is the following: First, identify pages that you are interested in and find all sessions that contain these pages. Then run frequent pattern mining algorithms on the filtered session collection. The output should contain more relevant patterns with higher support. This method provides a simple way to analyze use cases more separately and thoroughly.

We conducted the experiment and focused only on sessions that contained the page \emph{inimesed/Instituudid}. The results did not reveal anything interesting, but helped us find more patterns about the page than we had found before. For example, a frequent pattern with $40\%$ support among filtered sessions was discovered that showed how in some cases users tend to reload the front page before arriving on \emph{inimesed/Instituudid}.

\section{Summary}
We have shown a practical example of how web analytics can be taken beyond Google analytics.
More work could be done by researching outliers detection and and pattern significance measures.
Sequential patterns decorated with times helped us identify a hop-page without even clicking on web site.









\bibliographystyle{alpha}
\bibliography{bibliography}
\end{document}
